{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "features_all = fetch_20newsgroups(subset='all').data\n",
    "targets_all = fetch_20newsgroups(subset='all').target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import re\n",
    "import os\n",
    "\n",
    "# drop  \\, \", '\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for text in features_all:\n",
    "\n",
    "    text = clean_str(text.encode('ascii','ignore').decode(\"utf-8\"))\n",
    "    \n",
    "    sentences = tokenize.sent_tokenize(text) # list of string senteces\n",
    "\n",
    "    reviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from: mamatha devineni ratnam <mr47+@andrew.cmu.edu>\\nsubject: pens fans reactions\\norganization: post office, carnegie mellon, pittsburgh, pa\\nlines: 12\\nnntp-posting-host: po4.andrew.cmu.edu\\n\\n\\n\\ni am sure some bashers of pens fans are pretty confused about the lack\\nof any kind of posts about the recent pens massacre of the devils.', 'actually,\\ni am  bit puzzled too and a bit relieved.', 'however, i am going to put an end\\nto non-pittsburghers relief with a bit of praise for the pens.', 'man, they\\nare killing those devils worse than i thought.', 'jagr just showed you why\\nhe is much better than his regular season stats.', 'he is also a lot\\nfo fun to watch in the playoffs.', 'bowman should let jagr have a lot of\\nfun in the next couple of games since the pens are going to beat the pulp out of jersey anyway.', 'i was very disappointed not to see the islanders lose the final\\nregular season game.', 'pens rule!!', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reviews[0])\n",
    "len(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "reviews_clean = [[normalize_string(sent) for sent in doc if normalize_string(sent)] for doc in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from mamatha devineni ratnam mr47 andrew cmu edu subject pens fans reactions organization post office carnegie mellon pittsburgh pa lines 12 nntp posting host po4 andrew cmu edu i am sure some bashers of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils', 'actually i am bit puzzled too and a bit relieved', 'however i am going to put an end to non pittsburghers relief with a bit of praise for the pens', 'man they are killing those devils worse than i thought', 'jagr just showed you why he is much better than his regular season stats', 'he is also a lot fo fun to watch in the playoffs', 'bowman should let jagr have a lot of fun in the next couple of games since the pens are going to beat the pulp out of jersey anyway', 'i was very disappointed not to see the islanders lose the final regular season game', 'pens rule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reviews_clean[0])\n",
    "len(reviews_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "class Lang():\n",
    "    def __init__(self, docs, stoplist, min_count=30):\n",
    "        self.docs = docs\n",
    "        self.stoplist = stoplist\n",
    "        self.min_count = min_count\n",
    "        self.word2idx = {\"<PAD>\": 0}\n",
    "        self.idx2word = {0: \"<PAD>\"}\n",
    "        self.n_words = self.process_sents()\n",
    "\n",
    "    def process_sents(self):\n",
    "        words = []\n",
    "        for doc in self.docs:\n",
    "            for sent in doc:\n",
    "                words += sent.split(' ')\n",
    "\n",
    "        cc = 1\n",
    "        counter = Counter(words)\n",
    "        for word, num in counter.items():\n",
    "            if num > self.min_count and word not in self.stoplist:\n",
    "                self.word2idx[word] = cc\n",
    "                self.idx2word[cc] = word\n",
    "                cc += 1\n",
    "        return cc\n",
    "    \n",
    "input_lang = Lang(reviews_clean, stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12706"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert input_lang.n_words == len(input_lang.word2idx)\n",
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\n",
      "andrew\n",
      "cmu\n",
      "edu\n",
      "subject\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(input_lang.idx2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11851 words with pre-trained vector in vocab_size = 12706.\n"
     ]
    }
   ],
   "source": [
    "## load pre-trained word2vec\n",
    "with open('/data/charley/crawl-300d-2M.pkl', 'rb') as pf:\n",
    "    fb_w2v = pickle.load(pf)\n",
    "    \n",
    "embed_dim = 300\n",
    "embed_matrix = np.zeros((input_lang.n_words, embed_dim))\n",
    "flag = 0\n",
    "for word, idx in input_lang.word2idx.items():\n",
    "    word_vec = fb_w2v.get(word)\n",
    "    if word_vec is not None:\n",
    "        embed_matrix[idx] = word_vec\n",
    "        flag += 1\n",
    "print(\"There are {} words with pre-trained vector in vocab_size = {}.\".format(flag, len(input_lang.word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('EmbeddingMatrix_han', embed_matrix) # saved as EmbeddingMatrix.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12706, 300)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "9\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "input_docs = []\n",
    "for doc in reviews_clean:\n",
    "    doc_idx = [[input_lang.word2idx[word] for word in sent.split(' ') if word in input_lang.word2idx] \n",
    "               for sent in doc]\n",
    "    input_docs.append(doc_idx)\n",
    "print(len(input_docs)) # num of docs\n",
    "print(len(input_docs[0])) # num of sentences in doc-1\n",
    "print(len(input_docs[0][0])) # num of words in sentence-1 of doc-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2246, 310, 4, 1441, 302, 3917, 4338, 8, 512, 4339, 487, 1782, 587, 15, 1183, 306, 1056, 2410, 3810, 3, 469, 3810, 3, 147, 4340, 147, 163, 1176, 1067, 4341, 202, 588, 525, 2386, 409, 1381], [1062, 24, 1686, 24, 556, 809, 3994, 3311, 996, 4342], [1580, 202, 1406, 121, 4132, 4343, 30, 4344, 4345, 604, 456, 1118, 343], [4231, 4346, 4347, 4348, 578, 776, 2023, 3584, 4231], [4349, 498, 4350, 4351, 454, 4149, 4352, 4353, 4354, 4355, 4356, 4325, 2292, 3999, 4357, 2023, 342, 4132, 4343], [130, 2260, 4358, 2022, 3987, 371, 259, 4359, 787], [654, 121, 4360, 163, 4361, 4362, 4363], [2260]]\n"
     ]
    }
   ],
   "source": [
    "print(input_docs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2246, 310, 4, 1441, 302, 3917, 4338, 8, 512, 4339, 487, 1782, 587, 15, 1183, 306, 1056, 2410, 3810, 3, 469, 3810, 3, 147, 4340, 147, 163, 1176, 1067, 4341, 202, 588, 525, 2386, 409, 1381], [1062, 24, 1686, 24, 556, 809, 3994, 3311, 996, 4342], [1580, 202, 1406, 121, 4132, 4343, 30, 4344, 4345, 604, 456, 1118, 343], [4231, 4346, 4347, 4348, 578, 776, 2023, 3584, 4231], [4349, 498, 4350, 4351, 454, 4149, 4352, 4353, 4354, 4355, 4356, 4325, 2292, 3999, 4357, 2023, 342, 4132, 4343], [130, 2260, 4358, 2022, 3987, 371, 259, 4359, 787], [654, 121, 4360, 163, 4361, 4362, 4363], [2260]]\n"
     ]
    }
   ],
   "source": [
    "# drop sentencs without word \n",
    "input_docs_new = []\n",
    "for doc in input_docs:\n",
    "    input_docs_new.append([sent for sent in doc if sent])\n",
    "print(input_docs_new[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 289497)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths = [len(doc) for doc in input_docs_new]\n",
    "sent_lengths = [len(sent) for doc in input_docs_new for sent in doc]\n",
    "len(doc_lengths), len(sent_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.36119070359758 30.04536205310365 1 1186\n",
      "11.00158550865812 24.322507329258436 1 7031\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_info(doc_lengths):\n",
    "    print(np.mean(doc_lengths), np.std(doc_lengths), np.min(doc_lengths), np.max(doc_lengths))\n",
    "\n",
    "print_info(doc_lengths)\n",
    "print_info(sent_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 75, 60)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs = len(input_docs_new) # 18846\n",
    "max_sents = 75 # 15 + 30x2\n",
    "max_sent_length = 60\n",
    "\n",
    "data = np.zeros((num_docs, max_sents, max_sent_length), dtype='int32')\n",
    "sentences_per_document = np.zeros(num_docs, dtype='int32') # num of sentences of a doc\n",
    "words_per_sentence = np.zeros((num_docs, max_sents), dtype='int32')\n",
    "\n",
    "for i, doc in enumerate(input_docs_new):\n",
    "    doc_len = min(len(doc), max_sents)\n",
    "    doc = doc[:doc_len]\n",
    "    sentences_per_document[i] = doc_len\n",
    "    for j, sent in enumerate(doc):\n",
    "        sent_len = min(len(sent), max_sent_length)\n",
    "        data[i, j, :sent_len] = sent[:sent_len]\n",
    "        words_per_sentence[i, j] = sent_len\n",
    "        \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2246,  310,    4, 1441,  302, 3917, 4338,    8,  512, 4339],\n",
       "       [1062,   24, 1686,   24,  556,  809, 3994, 3311,  996, 4342],\n",
       "       [1580,  202, 1406,  121, 4132, 4343,   30, 4344, 4345,  604],\n",
       "       [4231, 4346, 4347, 4348,  578,  776, 2023, 3584, 4231,    0],\n",
       "       [4349,  498, 4350, 4351,  454, 4149, 4352, 4353, 4354, 4355],\n",
       "       [ 130, 2260, 4358, 2022, 3987,  371,  259, 4359,  787,    0],\n",
       "       [ 654,  121, 4360,  163, 4361, 4362, 4363,    0,    0,    0],\n",
       "       [2260,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100, :10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, array([36, 10, 13,  9, 19,  9,  7,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0], dtype=int32))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_per_document[100], words_per_sentence[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 75, 60) (11314,) (11314, 75) (11314,)\n"
     ]
    }
   ],
   "source": [
    "## split data\n",
    "x_train, sents_doc_train, words_sent_train, y_train = \\\n",
    "            data[:11314], sentences_per_document[:11314], words_per_sentence[:11314], targets_all[:11314]\n",
    "x_test, sents_doc_test, words_sent_test, y_test = \\\n",
    "            data[11314:], sentences_per_document[11314:], words_per_sentence[11314:], targets_all[11314:]\n",
    "print(x_train.shape, sents_doc_train.shape, words_sent_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "newspaper_han_data = {'trian': {1:x_train, 2:sents_doc_train, 3:words_sent_train, 4:y_train}, \n",
    "                      'test': {1:x_test, 2:sents_doc_test, 3:words_sent_test, 4:y_test},\n",
    "                      'lang': input_lang}\n",
    "\n",
    "with open('newspaper_han.pkl', 'wb') as f:\n",
    "    pickle.dump(newspaper_han_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsPaperDatasets(Dataset):\n",
    "    def __init__(self, src, src_sents, src_words, tgt):\n",
    "        self.src = src\n",
    "        self.src_sents = src_sents\n",
    "        self.src_words = src_words\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.src_sents[idx], self.src_words[idx], self.tgt[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 75, 60])\n",
      "tensor([4, 2, 9, 8], dtype=torch.int32)\n",
      "torch.Size([4, 75])\n",
      "tensor([ 4, 11, 19, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try_loader = torch.utils.data.DataLoader(\n",
    "                    NewsPaperDatasets(x_train[:16], sents_doc_train[:16], words_sent_train[:16], y_train[:16]),\n",
    "                    num_workers = 2,\n",
    "                    batch_size = 4,\n",
    "                    shuffle = True,\n",
    "                    drop_last = True)\n",
    "\n",
    "for batch in try_loader:\n",
    "    src, src_sents, src_words, tgt = batch\n",
    "    print(src.size())\n",
    "    print(src_sents)\n",
    "    print(src_words.size())\n",
    "    print(tgt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    NewsPaperDatasets(x_train, sents_doc_train, words_sent_train, y_train),\n",
    "                    num_workers = 2,\n",
    "                    batch_size = 64,\n",
    "                    shuffle = True,\n",
    "                    drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    NewsPaperDatasets(x_test, sents_doc_test, words_sent_test, y_test),\n",
    "                    num_workers = 2,\n",
    "                    batch_size = 64,\n",
    "                    shuffle = True,\n",
    "                    drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_HAN import *\n",
    "\n",
    "model = HierarchialAttentionNetwork(n_classes = 20, \n",
    "                                    vocab_size = input_lang.n_words, \n",
    "                                    emb_size = 100, \n",
    "                                    word_rnn_size = 128, \n",
    "                                    sentence_rnn_size = 128, \n",
    "                                    word_rnn_layers = 2,\n",
    "                                    sentence_rnn_layers = 2, \n",
    "                                    word_att_size = 128, \n",
    "                                    sentence_att_size = 128, \n",
    "                                    dropout = 0.5, \n",
    "                                    embed_weights = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchialAttentionNetwork(\n",
       "  (sentence_attention): SentenceAttention(\n",
       "    (word_attention): WordAttention(\n",
       "      (embeddings): Embedding(12706, 100)\n",
       "      (word_rnn): GRU(100, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "      (word_attention): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (word_context_vector): Linear(in_features=128, out_features=1, bias=False)\n",
       "      (dropout): Dropout(p=0.5)\n",
       "    )\n",
       "    (sentence_rnn): GRU(256, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "    (sentence_attention): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (sentence_context_vector): Linear(in_features=128, out_features=1, bias=False)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=20, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicat/anaconda3/envs/charley/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0030689239501953\n",
      "2.9946494102478027\n",
      "3.002408504486084\n",
      "3.0123629570007324\n",
      "3.008464813232422\n",
      "3.013972759246826\n",
      "3.0314745903015137\n",
      "3.0051140785217285\n",
      "2.9838218688964844\n",
      "2.9835832118988037\n",
      "3.0121264457702637\n",
      "3.006166458129883\n",
      "2.9987599849700928\n",
      "2.967792510986328\n",
      "3.021367073059082\n",
      "3.0165843963623047\n",
      "2.975843906402588\n",
      "3.0029866695404053\n",
      "2.987004280090332\n",
      "2.9833061695098877\n",
      "2.9648263454437256\n",
      "3.010542154312134\n",
      "2.9773077964782715\n",
      "3.0179946422576904\n",
      "2.936753749847412\n",
      "2.9322447776794434\n",
      "3.010312795639038\n",
      "2.998314380645752\n",
      "2.9591684341430664\n",
      "2.967848300933838\n",
      "2.937185525894165\n",
      "2.9367311000823975\n",
      "2.9368481636047363\n",
      "2.8597543239593506\n",
      "2.9494822025299072\n",
      "2.9266867637634277\n",
      "2.920616865158081\n",
      "2.8704466819763184\n",
      "2.9774718284606934\n",
      "2.9136242866516113\n",
      "2.977733850479126\n",
      "2.981370449066162\n",
      "3.0409085750579834\n",
      "2.8914735317230225\n",
      "2.860506772994995\n",
      "2.9757823944091797\n",
      "2.948040008544922\n",
      "3.08793044090271\n",
      "3.0192861557006836\n",
      "3.5264792442321777\n",
      "3.216702699661255\n",
      "3.179327964782715\n",
      "2.7487082481384277\n",
      "3.055070161819458\n",
      "2.931467056274414\n",
      "3.3935933113098145\n",
      "3.3650405406951904\n",
      "3.3069114685058594\n",
      "3.8250112533569336\n",
      "3.199038028717041\n",
      "3.6688597202301025\n",
      "3.3613810539245605\n",
      "3.4905412197113037\n",
      "3.3778655529022217\n",
      "3.2572758197784424\n",
      "3.3254547119140625\n",
      "3.465177059173584\n",
      "3.5250282287597656\n",
      "3.3480076789855957\n",
      "3.1370849609375\n",
      "3.4124088287353516\n",
      "3.565603256225586\n",
      "3.3940324783325195\n",
      "3.34140944480896\n",
      "3.5839884281158447\n",
      "3.393749237060547\n",
      "3.317129611968994\n",
      "3.6041293144226074\n",
      "3.017021656036377\n",
      "3.1469483375549316\n",
      "2.8985023498535156\n",
      "3.0606930255889893\n",
      "3.0679354667663574\n",
      "3.2692883014678955\n",
      "3.553485155105591\n",
      "3.3992135524749756\n",
      "3.692110538482666\n",
      "3.3472588062286377\n",
      "3.585843086242676\n",
      "3.5881426334381104\n",
      "3.402597665786743\n",
      "3.3996143341064453\n",
      "3.4651808738708496\n",
      "3.0828025341033936\n",
      "3.235375165939331\n",
      "3.4612998962402344\n",
      "3.3017542362213135\n",
      "3.3467350006103516\n",
      "3.4802513122558594\n",
      "3.3626205921173096\n",
      "3.462357759475708\n",
      "3.4506940841674805\n",
      "3.6949727535247803\n",
      "3.9125287532806396\n",
      "3.7427825927734375\n",
      "3.349670886993408\n",
      "3.6588833332061768\n",
      "3.3755288124084473\n",
      "3.553340196609497\n",
      "3.2304601669311523\n",
      "3.0215747356414795\n",
      "3.0140204429626465\n",
      "3.2851390838623047\n",
      "3.2297141551971436\n",
      "3.5420682430267334\n",
      "3.5823378562927246\n",
      "4.009251594543457\n",
      "4.138977527618408\n",
      "4.430421829223633\n",
      "4.521905899047852\n",
      "3.846388339996338\n",
      "4.305021286010742\n",
      "3.8225438594818115\n",
      "3.7803826332092285\n",
      "3.7144651412963867\n",
      "3.7954695224761963\n",
      "3.3820908069610596\n",
      "3.113680124282837\n",
      "3.2242624759674072\n",
      "3.0760936737060547\n",
      "3.1713852882385254\n",
      "3.3023972511291504\n",
      "3.9237141609191895\n",
      "3.5156774520874023\n",
      "3.5103421211242676\n",
      "3.7690460681915283\n",
      "3.6452763080596924\n",
      "3.8814425468444824\n",
      "3.8616983890533447\n",
      "3.5230135917663574\n",
      "3.818645477294922\n",
      "3.6604979038238525\n",
      "3.9636828899383545\n",
      "3.527587652206421\n",
      "3.5569043159484863\n",
      "3.388470411300659\n",
      "3.425156593322754\n",
      "3.462826728820801\n",
      "3.262650966644287\n",
      "3.2498154640197754\n",
      "3.1364803314208984\n",
      "3.2140731811523438\n",
      "3.2348458766937256\n",
      "3.031850814819336\n",
      "3.4272916316986084\n",
      "3.010821580886841\n",
      "3.271667242050171\n",
      "3.471024990081787\n",
      "3.5782322883605957\n",
      "3.6178135871887207\n",
      "3.7457430362701416\n",
      "3.767266273498535\n",
      "3.550069808959961\n",
      "3.7969226837158203\n",
      "3.961360454559326\n",
      "3.636564254760742\n",
      "3.7755868434906006\n",
      "3.363020181655884\n",
      "3.495434284210205\n",
      "3.29866623878479\n",
      "3.6044352054595947\n",
      "3.371006488800049\n",
      "3.3700032234191895\n",
      "3.3327393531799316\n",
      "3.0548853874206543\n",
      "2.9905858039855957\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    src, src_sents, src_words, tgt = [subtensor.long().to(device) for subtensor in batch]\n",
    "    outputs, _, _ = model(src, src_sents, src_words)\n",
    "    loss = criterion(outputs, tgt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charley",
   "language": "python",
   "name": "charley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
